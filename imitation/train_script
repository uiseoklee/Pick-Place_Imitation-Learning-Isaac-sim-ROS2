#!/usr/bin/env python

import os
from common.config import DiffusionConfig
from common.diffusion_policy import DiffusionPolicy
from common.dataset import LeRobotDataset


def main(root, epoch):
    print(root)
    if root is None:
        print('[ERROR] Must specified path to data!')
        return
    

    repo_id = 'test'
    from pathlib import Path
    import torch
    import matplotlib.pyplot as plt
    from datetime import datetime

    # Create a directory to store the training checkpoint.
    output_directory = Path("imitation/outputs/train")
 
    output_directory.mkdir(parents=True, exist_ok=True)

    training_steps = epoch
    device = torch.device("cuda")
    log_freq = 20

    # Set up the dataset.
    delta_timestamps = {
        "observation.image": [-0.2, -0.1, 0.0],
        "observation.state": [-0.2, -0.1, 0.0],
        "action": [-0.1, 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4],
    }
    dataset = LeRobotDataset(repo_id, delta_timestamps=delta_timestamps, root=root)
    print(dataset)
    print(dataset.hf_dataset)
    
    cfg = DiffusionConfig()
    # print('*** ', cfg)
    policy = DiffusionPolicy(cfg, dataset_stats=dataset.stats)
    policy.train()
    policy.to(device)
    
    optimizer = torch.optim.Adam(policy.parameters(), lr=1e-4)
    
    # Create dataloader for offline training.
    dataloader = torch.utils.data.DataLoader(
        dataset,
        num_workers=4,
        batch_size=8,
        shuffle=True,
        pin_memory=device != torch.device("cpu"),
        drop_last=True,
    )

    

    # Run training loop.
    step = 0
    done = False
    train_losses = []
    while not done:
        for batch in dataloader:
            batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}
            
            output_dict = policy.forward(batch)
            loss = output_dict["loss"]
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            if step % log_freq == 0:
                print(f"step: {step} loss: {loss.item():.3f}")
            step += 1

            train_losses.append(loss.item())

            if step >= training_steps:
                done = True
                break

    # Plot and save loss function
    plt.plot(train_losses, label='Training loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Loss Function')
    plt.legend()

    time_now = datetime.now()
    loss_path = 'imitation/outputs/losses/'
    if not os.path.exists(loss_path):
        os.mkdir(loss_path)

    name = loss_path + 'loss_' + time_now.strftime("%Y_%m_%d_%H_%M_%S") + '.png'
    plt.savefig(name, bbox_inches='tight', dpi=300)

    # Save a policy checkpoint.
    policy.save_pretrained(output_directory)



if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser(description='Compute stats')
    parser.add_argument('--path', type=str, help='Data path')
    parser.add_argument('--epoch', type=int, default=1000, help='Number of epoch')
    parsed_args = parser.parse_args()

    main(parsed_args.path, parsed_args.epoch)